import os
import sys
import argparse
import time
import random
import re
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional

import mysql.connector
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import os
import tempfile

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –¥–æ–º–∞—à–Ω–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
TEMP_DIR = os.path.join(os.path.expanduser("~"), ".playwright-tmp")
os.makedirs(TEMP_DIR, exist_ok=True)

# –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Ç—É–¥–∞
os.environ["PLAYWRIGHT_TMPDIR"] = TEMP_DIR
os.environ["TMPDIR"] = TEMP_DIR
os.environ["TEMP"] = TEMP_DIR
os.environ["TMP"] = TEMP_DIR
tempfile.tempdir = TEMP_DIR

print(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è Playwright: {TEMP_DIR}")

load_dotenv()

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –æ—à–∏–±–æ–∫ ===
TRACE_URL = "https://server.brain-project.online/trace.php"
NODE_NAME = os.getenv("NODE_NAME", "binance_news_parser")
EMAIL = os.getenv("ALERT_EMAIL", "vladyurjevitch@yandex.ru")

def send_error_trace(exc: Exception, script_name: str = "Binance_news.py"):
    logs = (
        f"Node: {NODE_NAME}\n"
        f"Script: {script_name}\n"
        f"Exception: {repr(exc)}\n\n"
        f"Traceback:\n{traceback.format_exc()}"
    )
    payload = {
        "url": "cli_script",
        "node": NODE_NAME,
        "email": EMAIL,
        "logs": logs,
    }
    print(f"\nüì§ [POST] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á—ë—Ç –æ–± –æ—à–∏–±–∫–µ –Ω–∞ {TRACE_URL}")
    try:
        import requests
        response = requests.post(TRACE_URL, data=payload, timeout=10)
        print(f"‚úÖ [POST] –£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ! –°—Ç–∞—Ç—É—Å: {response.status_code}")
    except Exception as e:
        print(f"‚ö†Ô∏è [POST] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ—Ç—á—ë—Ç: {e}")

# === –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ ===
parser = argparse.ArgumentParser(description="Binance Square News ‚Üí MySQL")
parser.add_argument("table_name", help="–ò–º—è —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤ –ë–î")
parser.add_argument("host", nargs="?", default=os.getenv("DB_HOST"), help="–•–æ—Å—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
parser.add_argument("port", nargs="?", default=os.getenv("DB_PORT", "3306"), help="–ü–æ—Ä—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
parser.add_argument("user", nargs="?", default=os.getenv("DB_USER"), help="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ë–î")
parser.add_argument("password", nargs="?", default=os.getenv("DB_PASSWORD"), help="–ü–∞—Ä–æ–ª—å –ë–î")
parser.add_argument("database", nargs="?", default=os.getenv("DB_NAME"), help="–ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
args = parser.parse_args()

if not all([args.host, args.user, args.password, args.database]):
    print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î")
    sys.exit(1)

DB_CONFIG = {
    'host': args.host,
    'port': int(args.port),
    'user': args.user,
    'password': args.password,
    'database': args.database,
}

# ---------- CONFIG ----------
SETTINGS = {
    "base_url": "https://www.binance.com/en/square/news/all",
    "scrolls": 3,                       # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∫—Ä—É—Ç–æ–∫ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    "pause_after_click": 5.0,            # –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞ (—Å–µ–∫)
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "locale": "en-US",
    "timeout": 60000,
}

def log(msg: str) -> None:
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

# ---------- DB ----------
class DB:
    def __init__(self, table_name: str):
        self.table_name = table_name

    def get_db_connection(self):
        return mysql.connector.connect(**DB_CONFIG)

    def ensure_table(self) -> None:
        with self.get_db_connection() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                CREATE TABLE IF NOT EXISTS `{self.table_name}` (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    link VARCHAR(255) NOT NULL UNIQUE,
                    title TEXT,
                    full_text TEXT,
                    preview TEXT,
                    date VARCHAR(32),
                    author VARCHAR(100),
                    inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_date (date)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
            """)
            conn.commit()

    def upsert_single(self, row: Dict[str, Any]) -> bool:
        """
        –í—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç (–Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç).
        """
        sql = f"""
        INSERT IGNORE INTO `{self.table_name}` (link, title, full_text, preview, date, author)
        VALUES (%(link)s, %(title)s, %(full_text)s, %(preview)s, %(date)s, %(author)s)
        """
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(sql, row)
            affected = cursor.rowcount
            conn.commit()
            return affected > 0

# ---------- –ü–∞—Ä—Å–∏–Ω–≥ ----------
def extract_full_text(page) -> tuple[str, str, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, –∞–≤—Ç–æ—Ä–∞ –∏ –¥–∞—Ç—É —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (full_text, author, date_str).
    """
    html = page.content()
    soup = BeautifulSoup(html, "html.parser")

    # 1. –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    selectors = [
        "div[class*='post-content']",
        "div[class*='article-body']",
        "div[class*='content-body']",
        "div[class*='body']",
        "article",
        "div[class*='text']",
        "div[class*='prose']",
        "main p",
    ]
    full_text = ""
    for sel in selectors:
        block = soup.select_one(sel)
        if block:
            # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä
            for unwanted in block.select("script, style, header, footer, nav, button, .login, .signup"):
                unwanted.decompose()
            full_text = block.get_text(separator="\n", strip=True)
            break

    if not full_text:
        # fallback: –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = soup.find_all("p")
        full_text = "\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30])

    full_text = re.sub(r'\s+', ' ', full_text).strip()[:10000]  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ 10k —Å–∏–º–≤–æ–ª–æ–≤

    # 2. –ê–≤—Ç–æ—Ä
    author = "Binance Square"
    author_elem = soup.find(class_=re.compile(r"author|username|creator|byline"))
    if author_elem:
        author = author_elem.get_text(strip=True)
    else:
        match = re.search(r'By\s+([A-Za-z\s]+?)(?:\s+on|\s+¬∑)', html, re.I)
        if match:
            author = match.group(1).strip()

    # 3. –î–∞—Ç–∞
    date_str = datetime.now().strftime('%Y-%m-%d %H:%M')
    time_tag = soup.find("time")
    if time_tag and time_tag.get("datetime"):
        dt = time_tag["datetime"]
        date_str = dt[:10] + " " + dt[11:16] if "T" in dt else dt

    return full_text, author, date_str

def collect_links(page, max_scrolls: int) -> Dict[str, tuple[str, str]]:
    log(f"–°–∫—Ä–æ–ª–ª–∏–º {max_scrolls} —Ä–∞–∑...")
    for i in range(max_scrolls):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(SETTINGS["pause_after_click"] + random.uniform(-0.5, 1.5))
        log(f"  —Å–∫—Ä–æ–ª–ª {i+1}/{max_scrolls}")

    page.wait_for_timeout(5000)  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –¥–æ–≥—Ä—É–∑–∫–∏
    html = page.content()

    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /en/square/post/ID
    post_links = re.findall(r'href="([^"]*/square/post/(\d+))"[^>]*>([^<]+)</a>', html, re.I | re.S)
    unique = {}
    for link_part, post_id, title in post_links:
        full_link = f"https://www.binance.com{link_part}" if link_part.startswith('/') else link_part
        clean_title = re.sub(r'\s+', ' ', title.strip())
        if post_id not in unique and len(clean_title) > 15:
            unique[post_id] = (full_link, clean_title)

    log(f"–ù–∞–π–¥–µ–Ω–æ {len(unique)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    return unique

# ---------- MAIN ----------
def main() -> int:
    db = DB(args.table_name)
    db.ensure_table()
    log(f"–¶–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞: {args.database}.{args.table_name}")
    log(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∫—Ä—É—Ç–æ–∫: {SETTINGS['scrolls']}")

    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        raise SystemExit("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ playwright: pip install playwright && playwright install chromium")

    processed_total = 0
    seen_total = 0

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent=SETTINGS["user_agent"],
            locale=SETTINGS["locale"],
            timezone_id="UTC",
        )
        page = context.new_page()

        log(f"–û—Ç–∫—Ä—ã–≤–∞—é {SETTINGS['base_url']}")
        page.goto(SETTINGS["base_url"], timeout=SETTINGS["timeout"])
        log("–û–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ / –∫–∞–ø—á–∏...")
        page.wait_for_timeout(10000)

        # –°–±–æ—Ä —Å—Å—ã–ª–æ–∫
        links_map = collect_links(page, SETTINGS["scrolls"])

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
        for i, (post_id, (link, title)) in enumerate(links_map.items(), 1):
            log(f"[{i}/{len(links_map)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {title[:70]}...")
            try:
                page.goto(link, timeout=30000)
                page.wait_for_timeout(SETTINGS["pause_after_click"] * 1000)

                full_text, author, date_str = extract_full_text(page)

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏
                row = {
                    "link": link,
                    "title": title,
                    "full_text": full_text,
                    "preview": (full_text[:300] + "...") if full_text else title[:200] + "...",
                    "date": date_str,
                    "author": author,
                }

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                if db.upsert_single(row):
                    processed_total += 1

                seen_total += 1
                log(f"  ‚úì –¥–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ, –≤—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_total}")

                # –í–æ–∑–≤—Ä–∞—Ç –Ω–∞ –≥–ª–∞–≤–Ω—É—é
                page.go_back(timeout=15000)
                page.wait_for_timeout(2000 + random.uniform(0, 2))

            except Exception as e:
                log(f"  ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
                try:
                    page.go_back()
                except:
                    pass

        browser.close()

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    with db.get_db_connection() as conn:
        cur = conn.cursor()
        cur.execute(f"SELECT COUNT(*) FROM `{args.table_name}` WHERE full_text IS NOT NULL AND full_text != ''")
        count_with_text = cur.fetchone()[0]
        log(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —Å —Ç–µ–∫—Å—Ç–æ–º: {count_with_text}")

    log(f"–ì–æ—Ç–æ–≤–æ. –ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ={seen_total}, –î–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ={processed_total}")
    return 0

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except SystemExit:
        pass
    except KeyboardInterrupt:
        print("\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e!r}")
        send_error_trace(e)
        sys.exit(1)