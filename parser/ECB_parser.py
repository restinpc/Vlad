#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import argparse
import time
import zipfile
import io
import traceback
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import feedparser
from dateutil import parser as date_parser
import mysql.connector
from mysql.connector import Error
from dotenv import load_dotenv

load_dotenv()

TRACE_URL = "https://server.brain-project.online/trace.php"
NODE_NAME = os.getenv("NODE_NAME", "ecb_parser")
EMAIL = os.getenv("ALERT_EMAIL", "vladyurjevitch@yandex.ru")

BASE_URL_RSS = "https://www.ecb.europa.eu/home/html/rss.en.html"
ZIP_URL = "https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip"
CSV_URL = "https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.csv"


def send_error_trace(exc: Exception):
    logs = f"Node: {NODE_NAME}\nScript: ECB_parser.py\nException: {repr(exc)}\n\nTraceback:\n{traceback.format_exc()}"
    try:
        requests.post(TRACE_URL, data={"url": "cli_script", "node": NODE_NAME, "email": EMAIL, "logs": logs},
                      timeout=10)
    except:
        pass


def download_and_read_zip_csv(url):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç ZIP-–∞—Ä—Ö–∏–≤ –ø–æ URL, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–µ–≥–æ CSV-—Ñ–∞–π–ª
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame.
    """
    local_zip = "eurofxref-hist.zip"
    csv_filename_in_zip = "eurofxref-hist.csv"  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –≤–Ω—É—Ç—Ä–∏ –∞—Ä—Ö–∏–≤–∞

    try:
        # 1. –°–∫–∞—á–∏–≤–∞–µ–º ZIP-–∞—Ä—Ö–∏–≤
        print(f"1. –°–∫–∞—á–∏–≤–∞—é –∞—Ä—Ö–∏–≤ –∏–∑: {url}")
        response = requests.get(url, timeout=15, stream=True)
        response.raise_for_status()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º ZIP –Ω–∞ –¥–∏—Å–∫ (—á—Ç–æ–±—ã –∏–º–µ—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –∫–æ–ø–∏—é)
        with open(local_zip, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"   –ê—Ä—Ö–∏–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫: {local_zip}")

        # 2. –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ZIP –∏ —á–∏—Ç–∞–µ–º CSV
        print(f"2. –ò–∑–≤–ª–µ–∫–∞—é '{csv_filename_in_zip}' –∏–∑ –∞—Ä—Ö–∏–≤–∞...")
        with zipfile.ZipFile(local_zip, 'r') as zf:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω—É–∂–Ω—ã–π —Ñ–∞–π–ª –≤ –∞—Ä—Ö–∏–≤–µ
            if csv_filename_in_zip not in zf.namelist():
                # –ï—Å–ª–∏ –∏–º—è –¥—Ä—É–≥–æ–µ, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π CSV —Ñ–∞–π–ª
                csv_files = [f for f in zf.namelist() if f.endswith('.csv')]
                if not csv_files:
                    raise Exception("–í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤.")
                csv_filename_in_zip = csv_files[0]
                print(f"   –ù–∞–π–¥–µ–Ω CSV —Ñ–∞–π–ª: {csv_filename_in_zip}")

            # –ß–∏—Ç–∞–µ–º CSV —Å—Ä–∞–∑—É –≤ pandas –∏–∑ –∞—Ä—Ö–∏–≤–∞ (–±–µ–∑ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤)
            with zf.open(csv_filename_in_zip) as csv_file:
                df = pd.read_csv(csv_file)

        # 3. –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        num_rows = df.shape[0]
        print(f"   ‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω, —Å—Ç—Ä–æ–∫: {num_rows}")
        print(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞: {df['Date'].max()}")

        if df['Date'].max() < '2026-01-01':
            raise ValueError(f"–î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ! Max –¥–∞—Ç–∞ {df['Date'].max()}")

        return df

    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
        raise
    except zipfile.BadZipFile:
        print("‚ùå –û—à–∏–±–∫–∞: —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è ZIP –∞—Ä—Ö–∏–≤–æ–º –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω.")
        raise
    except Exception as e:
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        raise


parser = argparse.ArgumentParser(description="ECB Parser: rates –∏–∑ ZIP/CSV + items —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º")
parser.add_argument("table_name", help="–ü—Ä–µ—Ñ–∏–∫—Å —Ç–∞–±–ª–∏—Ü (vlad, vlad_ecb_rates, vlad_ecb_items –∏ —Ç.–¥.)")
parser.add_argument("host", nargs="?", default=os.getenv("DB_HOST"))
parser.add_argument("port", nargs="?", default=os.getenv("DB_PORT", "3306"))
parser.add_argument("user", nargs="?", default=os.getenv("DB_USER"))
parser.add_argument("password", nargs="?", default=os.getenv("DB_PASSWORD"))
parser.add_argument("database", nargs="?", default=os.getenv("DB_NAME"))
args = parser.parse_args()

DB_CONFIG = {
    'host': args.host,
    'port': int(args.port),
    'user': args.user,
    'password': args.password,
    'database': args.database,
}


class ECBParser:
    def __init__(self, prefix: str):
        clean = prefix.split('_ecb_')[0].rstrip('_') if '_ecb_' in prefix else prefix
        self.prefix = clean or "vlad"

        p = prefix.lower()
        if any(w in p for w in ['rates', 'exchange', 'fxref', 'currency']):
            self.mode = "rates"
        elif 'items' in p:
            self.mode = "items"
        else:
            self.mode = "all"

        self.items_table = f"{self.prefix}_ecb_items" if self.mode in ("all", "items") else None
        self.rates_table = f"{self.prefix}_ecb_exchange_rates" if self.mode in ("all", "rates") else None

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"})
        self.init_db()

    def get_db_connection(self):
        return mysql.connector.connect(**DB_CONFIG)

    def init_db(self):
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            if self.rates_table:
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS `{self.rates_table}` (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        currency CHAR(3) NOT NULL,
                        rate_date DATE NOT NULL,
                        rate DECIMAL(22,12) NOT NULL,
                        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                        UNIQUE KEY unique_rate (currency, rate_date)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
                """)
                print(f"   ‚Üí –¢–∞–±–ª–∏—Ü–∞ {self.rates_table} ")

            if self.items_table:
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS `{self.items_table}` (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        feed_url VARCHAR(512) NOT NULL,
                        guid VARCHAR(512) NOT NULL UNIQUE,
                        feed_type VARCHAR(50),
                        title VARCHAR(1024),
                        link VARCHAR(1024),
                        published_at DATETIME,
                        description LONGTEXT,
                        full_text LONGTEXT,
                        scraped_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
                """)
                print(f"   ‚Üí –¢–∞–±–ª–∏—Ü–∞ {self.items_table} (—Å –∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–æ–º id)")

            conn.commit()
            print(f"‚úÖ –¢–∞–±–ª–∏—Ü—ã –≥–æ—Ç–æ–≤—ã (—Ä–µ–∂–∏–º {self.mode})")

    def run_rates(self):
        print("\nüìä –°–∫–∞—á–∏–≤–∞–µ–º –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –∫—É—Ä—Å–æ–≤ –∏–∑ eurofxref-hist.zip...")
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —á—Ç–µ–Ω–∏—è ZIP
            df = download_and_read_zip_csv(ZIP_URL)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ –¥–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ë–î
            print("\n3. –ü—Ä–µ–æ–±—Ä–∞–∑—É—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î...")
            df_melted = df.melt(id_vars=['Date'], var_name='currency', value_name='rate')
            df_melted['rate_date'] = pd.to_datetime(df_melted['Date'])
            df_melted = df_melted.drop('Date', axis=1)

            # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            df_melted = df_melted.dropna(subset=['rate'])

            print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {len(df_melted):,}")
            print(f"   –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {df_melted['rate_date'].min()} ‚Üí {df_melted['rate_date'].max()}")
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–ª—é—Ç: {df_melted['currency'].nunique()}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –ë–î –±–∞—Ç—á–∞–º–∏
            print("\n4. –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –≤ –ë–î...")
            batch_size = 10000
            total_inserted = 0
            total_updated = 0

            with self.get_db_connection() as conn:
                cursor = conn.cursor()

                for i in range(0, len(df_melted), batch_size):
                    batch = df_melted.iloc[i:i + batch_size]

                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                    values = [
                        (row['currency'], row['rate_date'].strftime('%Y-%m-%d'), float(row['rate']))
                        for _, row in batch.iterrows()
                    ]

                    # –í—Å—Ç–∞–≤–∫–∞ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–µ
                    cursor.executemany(f"""
                        INSERT INTO `{self.rates_table}` (currency, rate_date, rate)
                        VALUES (%s, %s, %s)
                        ON DUPLICATE KEY UPDATE
                            rate = VALUES(rate),
                            updated_at = CURRENT_TIMESTAMP
                    """, values)

                    conn.commit()
                    batch_inserted = cursor.rowcount
                    total_inserted += len(batch)
                    print(f"      –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_inserted:,} / {len(df_melted):,} –∑–∞–ø–∏—Å–µ–π...")

            print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_inserted:,} –∑–∞–ø–∏—Å–µ–π –≤ {self.rates_table}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ run_rates: {e}")
            traceback.print_exc()
            raise

    def fetch_rss_feeds(self):
        print(f"\nüì° –°–∫–∞–Ω–∏—Ä—É–µ–º RSS-—Å—Ç—Ä–∞–Ω–∏—Ü—É ‚Üí {BASE_URL_RSS}")
        resp = self.session.get(BASE_URL_RSS, timeout=30)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        feeds = []

        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —è–∑—ã–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –Ω–µ-RSS —Å—Å—ã–ª–∫–∏
        language_titles = {
            "–ë—ä–ª–≥–∞—Ä—Å–∫–∏", "ƒåe≈°tina", "Dansk", "Deutsch", "EŒªŒªŒ∑ŒΩŒπŒ∫Œ¨", "English", "Espa√±ol",
            "Eesti keel", "Suomi", "Fran√ßais", "Gaeilge", "Hrvatski", "Magyar", "Italiano",
            "Lietuvi≈≥", "Latvie≈°u", "Malti", "Nederlands", "Polski", "Portugu√™s", "Rom√¢nƒÉ",
            "Slovenƒçina", "Sloven≈°ƒçina", "Svenska"
        }

        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            title = a.get_text(strip=True) or "ECB Feed"

            # –ñ—ë—Å—Ç–∫–∏–π —Ñ–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–∏–µ RSS
            if not (
                href.startswith("/rss/fxref-") or          # –≤–∞–ª—é—Ç—ã
                "/rss/" in href and href.endswith((".html", ".rss", ".xml")) or
                href.endswith((".rss", ".xml"))
            ):
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–∑—ã–∫–æ–≤—ã–µ –∏ –º—É—Å–æ—Ä
            if re.match(r'^/rss\.[a-z]{2,3}\.html?$', href) or title in language_titles:
                continue

            if any(x in href.lower() for x in ["hist", "90d", "archive", ".zip", "pdf"]):
                continue

            full_url = urljoin("https://www.ecb.europa.eu", href)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ RSS (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ)
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å HEAD-–∑–∞–ø—Ä–æ—Å, –Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º —Ç–∞–∫
            feeds.append((full_url, title))

        feeds = list(dict.fromkeys(feeds))
        print(f" –ù–∞–π–¥–µ–Ω–æ {len(feeds)} —Ä–µ–∞–ª—å–Ω—ã—Ö RSS-—Ñ–∏–¥–æ–≤")
        for url, t in feeds[:10]:  # –ø–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print(f"   - {t}: {url}")
        return feeds

    def run_items(self):
        print("\nüì∞ –°–æ–±–∏—Ä–∞–µ–º RSS-—Å—Ç–∞—Ç—å–∏...")
        feeds = self.fetch_rss_feeds()
        count_new = 0

        for feed_url, title in feeds:
            print(f"\n   ‚Üì –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∏–¥: {title}")
            try:
                r = self.session.get(feed_url, timeout=45)
                r.raise_for_status()
                d = feedparser.parse(r.text)

                feed_type = self._get_feed_type(feed_url)

                with self.get_db_connection() as conn:
                    cursor = conn.cursor()

                    for entry in d.entries:
                        try:
                            guid = entry.get('id') or entry.get('guid') or entry.get('link')
                            if not guid: continue

                            published = None
                            for f in ['published', 'updated', 'dc_date', 'pubDate']:
                                if entry.get(f):
                                    try:
                                        published = date_parser.parse(entry.get(f))
                                        break
                                    except:
                                        continue

                            desc = entry.get('summary') or entry.get('description') or ""
                            if isinstance(desc, dict) and 'value' in desc:
                                desc = desc['value']

                            link = entry.get('link')
                            full_text = None

                            if link:
                                try:
                                    html_r = self.session.get(link, timeout=30)
                                    html_r.raise_for_status()
                                    soup = BeautifulSoup(html_r.text, 'html.parser')

                                    for tag in soup(['header', 'footer', 'nav', 'aside', 'script', 'style', 'form']):
                                        tag.decompose()

                                    content = soup.find('main') or soup.find('article') or \
                                              soup.find('div', class_=['content', 'article', 'rte', 'ecb-article'])
                                    if content:
                                        full_text = content.get_text(separator='\n', strip=True)
                                    else:
                                        full_text = soup.get_text(separator='\n', strip=True)[:200000]

                                except Exception as e:
                                    print(f"        –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç–∞—Ç—å—é {link}: {e}")

                            cursor.execute(f"""
                                INSERT INTO `{self.items_table}` 
                                (feed_url, guid, feed_type, title, link, published_at, description, full_text)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                                ON DUPLICATE KEY UPDATE 
                                    title=VALUES(title),
                                    published_at=VALUES(published_at),
                                    description=VALUES(description),
                                    full_text=VALUES(full_text),
                                    scraped_at=NOW()
                            """, (
                            feed_url, guid, feed_type, entry.get('title'), link, published, desc[:50000], full_text))

                            if cursor.rowcount != 0:
                                count_new += 1

                        except Exception as e:
                            continue

                    conn.commit()
                    print(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(d.entries)} –∑–∞–ø–∏—Å–µ–π")

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
            time.sleep(1.5)

        print(f"\n‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ {count_new} —Å—Ç–∞—Ç–µ–π –≤ {self.items_table}")

    def _get_feed_type(self, url: str) -> str:
        u = url.lower()
        if 'fxref' in u: return 'exchange_rate'
        if any(x in u for x in ['press', 'pressreleases']): return 'press_release'
        if 'speech' in u or '/key/' in u: return 'speech'
        if 'blog' in u: return 'blog'
        if 'statpress' in u: return 'statistical_release'
        return 'other'

    def run(self):
        print(f"\nüöÄ ECB Parser –∑–∞–ø—É—â–µ–Ω | –ø—Ä–µ—Ñ–∏–∫—Å: {self.prefix} | —Ä–µ–∂–∏–º: {self.mode.upper()}")
        if self.mode in ("all", "rates"):
            self.run_rates()
        if self.mode in ("all", "items"):
            self.run_items()
        print("\nüèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    try:
        ECBParser(args.table_name).run()
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        send_error_trace(e)
        sys.exit(1)